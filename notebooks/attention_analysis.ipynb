{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HGWRIXrjMHKy",
        "outputId": "b2b4247a-c2e3-425e-a36e-84dcb55eb373"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".output_wrapper, .output {\n",
              "    display: flex !important;\n",
              "    align-items: center;\n",
              "    justify-content: center;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        ".output_wrapper, .output {\n",
        "    display: flex !important;\n",
        "    align-items: center;\n",
        "    justify-content: center;\n",
        "}\n",
        "</style>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Vision Transformer Attention Analysis\n",
        "#\n",
        "# ![Attention Visualization](https://i.imgur.com/Xg7XQ0T.png)\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# %%\n",
        "# %%capture\n",
        "# !pip install torchcam\n",
        "\n",
        "# %%\n",
        "from models.hybrid_vit import HybridViT\n",
        "from models.utils import load_checkpoint\n",
        "from data.cifar10 import CIFAR10DataModule\n",
        "from torchcam.methods import GradCAM\n",
        "from torchcam.utils import overlay_mask\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Model and Data Loading\n",
        "\n",
        "# %%\n",
        "# Initialize model\n",
        "model = HybridViT(dim=256, depth=6, heads=8)\n",
        "model = load_checkpoint(model, \"checkpoints/best_model.pth\")\n",
        "model.eval();\n",
        "\n",
        "# Initialize datamodule\n",
        "dm = CIFAR10DataModule(batch_size=128)\n",
        "dm.setup()\n",
        "\n",
        "# Class labels\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Attention Head Visualization\n",
        "\n",
        "# %%\n",
        "def visualize_attention_heads(sample, layer_idx=0, n_heads=8):\n",
        "    with torch.no_grad():\n",
        "        _, attn_maps = model(sample.unsqueeze(0), return_attn=True)\n",
        "\n",
        "    attn = attn_maps[layer_idx][0]  # [heads, seq_len, seq_len]\n",
        "\n",
        "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    for head_idx in range(n_heads):\n",
        "        ax = axs[head_idx//4, head_idx%4]\n",
        "        head_attn = attn[head_idx, 0, 1:].reshape(4, 4)\n",
        "        ax.imshow(head_attn, cmap='viridis')\n",
        "        ax.set_title(f'Head {head_idx+1}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# %%\n",
        "# Get sample\n",
        "sample, label = dm.val_dataloader().dataset[42]\n",
        "plt.imshow(sample.permute(1, 2, 0))\n",
        "plt.title(f\"True: {classes[label]}\");\n",
        "plt.axis('off');\n",
        "\n",
        "# %%\n",
        "# Visualize first layer attention heads\n",
        "_ = visualize_attention_heads(sample, layer_idx=0)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Grad-CAM Visualization\n",
        "\n",
        "# %%\n",
        "class ViTCAM(GradCAM):\n",
        "    \"\"\"Custom GradCAM implementation for Hybrid ViT\"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layer):\n",
        "        super().__init__(model, target_layer)\n",
        "\n",
        "    def forward(self, x, class_idx=None):\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        features, _ = self.model(x, return_attn=True)\n",
        "        logits = features if class_idx is None else features[:, class_idx]\n",
        "\n",
        "        # Backward hook\n",
        "        self.hook_g = torch.zeros_like(features)\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.hook_g += grad_out[0].detach()\n",
        "\n",
        "        self.hook = self.target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "        # Backprop\n",
        "        logits.sum().backward()\n",
        "        self.hook.remove()\n",
        "\n",
        "        # Get CAM\n",
        "        cam = self.hook_g[0].mean(1).mean(1)\n",
        "        cam = torch.relu(cam)\n",
        "\n",
        "        return cam\n",
        "\n",
        "# %%\n",
        "# Initialize Grad-CAM\n",
        "target_layer = model.cnn_backbone[-3]  # Last CNN layer\n",
        "cam_extractor = ViTCAM(model, target_layer)\n",
        "\n",
        "# %%\n",
        "# Generate CAM\n",
        "sample_tensor = sample.unsqueeze(0)\n",
        "cam = cam_extractor(sample_tensor)\n",
        "\n",
        "# Overlay on image\n",
        "result = overlay_mask(\n",
        "    T.ToPILImage()(sample),\n",
        "    T.ToPILImage()(cam.unsqueeze(0)),\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "plt.imshow(sample.permute(1, 2, 0))\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(result)\n",
        "plt.title(\"Grad-CAM Visualization\")\n",
        "plt.axis('off');\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Confusion Matrix Analysis\n",
        "\n",
        "# %%\n",
        "def generate_confusion_matrix(model, datamodule):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(datamodule.val_dataloader()):\n",
        "            x, y = batch\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return cm\n",
        "\n",
        "# %%\n",
        "# Generate and plot confusion matrix\n",
        "cm = generate_confusion_matrix(model, dm)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix');\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Attention Pattern Evolution\n",
        "\n",
        "# %%\n",
        "def plot_attention_evolution(sample):\n",
        "    with torch.no_grad():\n",
        "        _, attn_maps = model(sample.unsqueeze(0), return_attn=True)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    for layer_idx in range(6):\n",
        "        ax = axs[layer_idx//3, layer_idx%3]\n",
        "        layer_attn = attn_maps[layer_idx][0, :, 0, 1:].mean(0)\n",
        "        ax.imshow(layer_attn.reshape(4, 4), cmap='viridis')\n",
        "        ax.set_title(f'Layer {layer_idx+1}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# %%\n",
        "plot_attention_evolution(sample)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. t-SNE Feature Visualization\n",
        "\n",
        "# %%\n",
        "def visualize_tsne(model, datamodule, n_samples=1000):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(datamodule.val_dataloader()):\n",
        "            cls_token = model(x)[:, 0]\n",
        "            features.append(cls_token)\n",
        "            labels.append(y)\n",
        "            if idx * 128 >= n_samples:\n",
        "                break\n",
        "\n",
        "    features = torch.cat(features)[:n_samples]\n",
        "    labels = torch.cat(labels)[:n_samples]\n",
        "\n",
        "    # t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=30)\n",
        "    embeddings = tsne.fit_transform(features.cpu().numpy())\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1],\n",
        "                         c=labels.cpu(), cmap='tab10', alpha=0.6)\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
        "    plt.title('t-SNE Visualization of CLS Token Embeddings');\n",
        "\n",
        "# %%\n",
        "visualize_tsne(model, dm)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Quantitative Attention Analysis\n",
        "\n",
        "# %%\n",
        "def calculate_attention_entropy(model, datamodule):\n",
        "    entropies = torch.zeros(6, 8)  # layers x heads\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in tqdm(datamodule.val_dataloader()):\n",
        "            _, attn_maps = model(x, return_attn=True)\n",
        "\n",
        "            for layer in range(6):\n",
        "                attn = attn_maps[layer][:, :, 0, 1:]  # [B, heads, seq]\n",
        "                prob = attn.softmax(dim=-1)\n",
        "                entropy = (-prob * prob.log()).sum(-1)  # [B, heads]\n",
        "                entropies[layer] += entropy.mean(0)\n",
        "\n",
        "    entropies /= len(datamodule.val_dataloader())\n",
        "    return entropies\n",
        "\n",
        "# %%\n",
        "entropies = calculate_attention_entropy(model, dm)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(entropies.numpy(), annot=True, fmt=\".2f\",\n",
        "            xticklabels=[f\"Head {i+1}\" for i in range(8)],\n",
        "            yticklabels=[f\"Layer {i+1}\" for i in range(6)])\n",
        "plt.title(\"Attention Entropy Across Layers/Heads (nats)\");\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "IPnqFtgaML9K",
        "outputId": "5ebe9ab3-4b2c-4447-f43e-cdf32dc264d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fcf6d030cfa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybrid_vit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHybridViT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifar10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCIFAR10DataModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}